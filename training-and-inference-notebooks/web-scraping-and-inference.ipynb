{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCPZ8XevEszm71stWQ5/IN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eteitelbaum/code-satp/blob/Fall-2024/web-scraping-and-inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping**"
      ],
      "metadata": {
        "id": "cXsyPYsZs9tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_satp_data(base_url, years, months):\n",
        "    data = []\n",
        "    for year in years:\n",
        "      for month in months:\n",
        "          url = f\"{base_url}-{month}-{year}\"\n",
        "          print(f\"Scraping: {url}\")\n",
        "          response = requests.get(url)\n",
        "          if response.status_code != 200:\n",
        "              print(f\"Failed to fetch data for {month}: {response.status_code}\")\n",
        "              continue\n",
        "\n",
        "          soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "          # Extract incident details and dates\n",
        "          coverpage_news = soup.find_all('div', class_='more')  # Incidents\n",
        "          coverpage_date = soup.find_all('td', style=\"width: 15%;\")  # Dates\n",
        "\n",
        "          # Validate counts of incidents and dates\n",
        "          if len(coverpage_news) != len(coverpage_date):\n",
        "              print(f\"Warning: Mismatch in dates ({len(coverpage_date)}) and incidents ({len(coverpage_news)}) for {month}.\")\n",
        "              continue\n",
        "\n",
        "          # Group incidents by date to track the nn counter\n",
        "          incidents_by_date = {}\n",
        "\n",
        "          # Iterate through the extracted dates and incidents\n",
        "          for date, incident in zip(coverpage_date, coverpage_news):\n",
        "              # Clean and format the incident summary\n",
        "              incident_summary = incident.get_text().strip()\n",
        "              incident_summary = re.sub(r\"\\s+\", \" \", incident_summary)  # Remove extra whitespace\n",
        "              incident_summary = incident_summary.replace(\"Read less...\", \"\")  # Remove \"Read less...\"\n",
        "\n",
        "              # Clean and format the date\n",
        "              raw_date = date.get_text().strip()\n",
        "              day = raw_date.split('-')[-1].strip()\n",
        "              month_number = f\"{months.index(month) + 1:02}\"  # Convert month name to two-digit number\n",
        "              formatted_date = f\"{year}-{month_number}-{day.zfill(2)}\"\n",
        "\n",
        "              # Track the nn counter for this date\n",
        "              if formatted_date not in incidents_by_date:\n",
        "                  incidents_by_date[formatted_date] = 0\n",
        "              incidents_by_date[formatted_date] += 1\n",
        "\n",
        "              # Generate the incident number in mmddyynn format\n",
        "              nn = f\"{incidents_by_date[formatted_date]:02}\"  # Increment counter for each summary\n",
        "              incident_number = f\"I{month_number}{day.zfill(2)}{year[-2:]}{nn}\"\n",
        "              # incident_number = int(incident_number)\n",
        "\n",
        "              # Append to the data list\n",
        "              data.append({\n",
        "                  \"Incident_Number\": incident_number,\n",
        "                  \"Date\": formatted_date,\n",
        "                  \"Incident_Summary\": incident_summary\n",
        "              })\n",
        "\n",
        "      # Convert the data to a pandas DataFrame\n",
        "    return pd.DataFrame(data), len(data)"
      ],
      "metadata": {
        "id": "4OuIMCCAryVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Main function\n",
        "def scrape_save(years, months):\n",
        "    base_url = \"https://www.satp.org/terrorist-activity/india-maoistinsurgency\"\n",
        "    # Scrape data\n",
        "    satp_data,l = scrape_satp_data(base_url, years, months)\n",
        "    print(f\"Total Incidents Scraped: {l}\")\n",
        "    # Save to Google Sheets\n",
        "    # save_to_google_sheets(scraped_data, \"SATP_Data\", \"raw_zone_incident_summaries\")\n",
        "\n",
        "\n",
        "years = [\"2017\",\"2018\"]\n",
        "months = [\"Jan\",\"Feb\"]\n",
        "#months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
        "\n",
        "scrape_save(years, months)\n"
      ],
      "metadata": {
        "id": "wcEmGvrt7TrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c518527a-8607-4312-afa1-5273ebe27d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.satp.org/terrorist-activity/india-maoistinsurgency-Jan-2017\n",
            "Scraping: https://www.satp.org/terrorist-activity/india-maoistinsurgency-Feb-2017\n",
            "Scraping: https://www.satp.org/terrorist-activity/india-maoistinsurgency-Jan-2018\n",
            "Scraping: https://www.satp.org/terrorist-activity/india-maoistinsurgency-Feb-2018\n",
            "Total Incidents Scraped: 484\n",
            "No new incidents found to upload.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "DuReOO5HtFsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit for creating web apps\n",
        "import streamlit as st\n",
        "\n",
        "# Web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NLP with Transformers\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Google Sheets API\n",
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "# Utility modules\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "stUe_OHOroS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#                                                                       infer_perpetrator\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "perpetrator_model_path = \"perpetrator/distilBert\"  # Update with your actual path\n",
        "perpetrator_model = AutoModelForSequenceClassification.from_pretrained(perpetrator_model_path)\n",
        "perpetrator_tokenizer = AutoTokenizer.from_pretrained(perpetrator_model_path)\n",
        "\n",
        "perpetrator_model.to(device)\n",
        "\n",
        "def infer_perpetrator(summary):\n",
        "    inputs = perpetrator_tokenizer(summary, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = perpetrator_model(**inputs)\n",
        "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    label_map = {0:'Security', 1:'Maoist', 2:'Unknown'}\n",
        "\n",
        "    predicted_perpetrator = label_map.get(predicted_class, \"Unknown\")\n",
        "    perpetrator = {\n",
        "        'perpetrator': predicted_perpetrator\n",
        "    }\n",
        "    return perpetrator\n"
      ],
      "metadata": {
        "id": "H9h2ITmttRFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#                                                                       inference_action_type\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "action_model_path = \"action_type/distilbert_model\"\n",
        "action_tokenizer = AutoTokenizer.from_pretrained(action_model_path)\n",
        "action_model = AutoModelForSequenceClassification.from_pretrained(action_model_path)\n",
        "\n",
        "action_model.to(device)\n",
        "\n",
        "\n",
        "def inference_action_type(summary):\n",
        "    \"\"\"\n",
        "    Performs inference on an incident summary to predict action types.\n",
        "    Args:\n",
        "        summary: The incident summary text.\n",
        "    Returns:\n",
        "        A dictionary with action type labels as keys and their predicted probabilities (0 or 1) as values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input summary\n",
        "    inputs = action_tokenizer(summary, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = action_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.sigmoid(logits)  # Get probabilities using sigmoid\n",
        "\n",
        "    # Convert probabilities to binary predictions (0 or 1) using threshold\n",
        "    threshold = 0.5\n",
        "    predictions = (probs > threshold).squeeze().cpu().numpy().astype(int)\n",
        "\n",
        "    # Create a dictionary to store the results\n",
        "    labels = ['action_armed_assault', 'action_arrest', 'action_bombing', 'action_infrastructure', 'action_surrender', 'action_seizure', 'action_abduction']\n",
        "    results = dict(zip(labels, predictions))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Hq6aaFCjtVbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#                                                                       inference_target_type\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "target_model_path = \"target_type/distilBert\"\n",
        "target_tokenizer = AutoTokenizer.from_pretrained(target_model_path)\n",
        "target_model = AutoModelForSequenceClassification.from_pretrained(target_model_path)\n",
        "\n",
        "\n",
        "target_model.to(device)\n",
        "\n",
        "\n",
        "def inference_target_type(summary):\n",
        "    \"\"\"\n",
        "    Performs inference on an incident summary to predict target types.\n",
        "    Args:\n",
        "        summary: The incident summary text.\n",
        "    Returns:\n",
        "        A dictionary with target type labels as keys and their predicted probabilities (0 or 1) as values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input summary\n",
        "    inputs = target_tokenizer(summary, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = target_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.sigmoid(logits)  # Get probabilities using sigmoid\n",
        "\n",
        "    # Convert probabilities to binary predictions (0 or 1) using threshold\n",
        "    threshold = 0.5\n",
        "    predictions = (probs > threshold).squeeze().cpu().numpy().astype(int)\n",
        "\n",
        "    # Create a dictionary to store the results\n",
        "    labels = ['target_civilians', 'target_maoist', 'target_no_target', 'target_security', 'target_government']\n",
        "    results = dict(zip(labels, predictions))\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "XFT-JrL0tYSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#                                                                       predict_counts\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Load the tokenizer and model from the saved directory\n",
        "total_num_tokenizer = T5Tokenizer.from_pretrained('total_injuries-arrests-surrenders-fatalities-abducted/t5small_finetuned_model')\n",
        "total_num_model = T5ForConditionalGeneration.from_pretrained('total_injuries-arrests-surrenders-fatalities-abducted/t5small_finetuned_model')\n",
        "\n",
        "total_num_model.to(device)\n",
        "\n",
        "def extract_number(text):\n",
        "    match = re.search(r'\\b\\d+\\b', text)\n",
        "    if match:\n",
        "        return int(match.group())\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def predict_counts(incident_summary):\n",
        "    questions = [\n",
        "        (\"How many injuries occurred in the incident?\", \"total_injuries\"),\n",
        "        (\"How many arrests were made in the incident?\", \"total_arrests\"),\n",
        "        (\"How many people surrendered in the incident?\", \"total_surrenders\"),\n",
        "        (\"How many fatalities occurred in the incident?\", \"total_fatalities\"),\n",
        "        (\"How many people were abducted in the incident?\", \"total_abducted\")\n",
        "    ]\n",
        "    counts = {}\n",
        "    for question, label in questions:\n",
        "        input_text = f\"question: {question} context: {incident_summary}\"\n",
        "        input_ids = total_num_tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
        "        input_ids = input_ids.to(device)\n",
        "        outputs = total_num_model.generate(input_ids)\n",
        "        answer = total_num_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        count = extract_number(answer)\n",
        "        counts[label] = count\n",
        "    return counts\n"
      ],
      "metadata": {
        "id": "AheUdZEJtcK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#                                                                       predict_damage\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "damage_model_path = 'damage_details_extraction/t5base_finetuned_model'\n",
        "damage_model = T5ForConditionalGeneration.from_pretrained(damage_model_path)\n",
        "damage_tokenizer = T5Tokenizer.from_pretrained(damage_model_path)\n",
        "\n",
        "damage_model.to(device)\n",
        "\n",
        "\n",
        "def predict_damage(summary):\n",
        "    # Prepare the input text\n",
        "    input_text = f\"Extract the property damage value from the incident: {summary}\"\n",
        "    input_ids = damage_tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).input_ids\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Generate predictions\n",
        "    outputs = damage_model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the output\n",
        "    predicted_damage = damage_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    damage_predictions = {\n",
        "        'value_property_damage': predicted_damage\n",
        "    }\n",
        "    return damage_predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "kCQzcyyFtbEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#                                                                       get_location_details\n",
        "# ----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "location_model_path = 'location_context_extraction/t5base_finetuned_model'\n",
        "location_model = T5ForConditionalGeneration.from_pretrained(location_model_path)\n",
        "location_tokenizer = T5Tokenizer.from_pretrained(location_model_path)\n",
        "\n",
        "location_model.to(device)\n",
        "\n",
        "\n",
        "# Updated function to get location details including latitude and longitude\n",
        "def get_location_details(summary):\n",
        "    \"\"\"Given a list of location names, constructs a query, calls the Google Geocoding API,\n",
        "    and returns state, district, subdistrict, town/village, and latitude/longitude of the most specific level.\"\"\"\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"Extract the location of the incident: {summary}\"\n",
        "    input_ids = location_tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).input_ids\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "\n",
        "    # Generate predictions\n",
        "    outputs = location_model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the output\n",
        "    locations = location_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def remove_specific_key_location_words(locations):\n",
        "        words_to_remove = [\"police\", \"station\",\"dam\",\"river\",\"rivers\",\"forests\",\"forest\"]  # Add more words here\n",
        "        cleaned_locations = locations.lower()\n",
        "        for word in words_to_remove:\n",
        "            cleaned_locations = cleaned_locations.replace(word.lower(), \"\")\n",
        "        return cleaned_locations\n",
        "\n",
        "    locations = remove_specific_key_location_words(locations)\n",
        "\n",
        "    # Google Maps API key\n",
        "    API_KEY = st.secrets[\"googlemapsAPI\"]\n",
        "    GEOCODE_URL = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
        "\n",
        "\n",
        "    #query = ', '.join(locations)\n",
        "    params = {\n",
        "        'address': locations,\n",
        "        'key': API_KEY,\n",
        "        'components': 'country:IN'\n",
        "    }\n",
        "    response = requests.get(GEOCODE_URL, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error in API call: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    data = response.json()\n",
        "    if data['status'] != 'OK':\n",
        "        print(f\"Geocoding API error: {data['status']}\")\n",
        "        return {\n",
        "        'Extracted_Locations': locations,\n",
        "        'state': None,\n",
        "        'district': None,\n",
        "        'subdistrict': None,\n",
        "        'town_village': None,\n",
        "        'latitude': None,\n",
        "        'longitude': None,\n",
        "        'location_Level': \"API couldn't find the Extracted_Locations\"\n",
        "    }\n",
        "\n",
        "    # Initialize components\n",
        "    state = district = subdistrict = town_village = None\n",
        "    latitude = longitude = None\n",
        "    found_level = None  # Keep track of the most specific level found\n",
        "\n",
        "    # Iterate over results to find the most specific level\n",
        "    for result in data.get('results', []):\n",
        "        temp_state = temp_district = temp_subdistrict = temp_town_village = None\n",
        "        address_components = result['address_components']\n",
        "\n",
        "        # Map address components\n",
        "        for component in address_components:\n",
        "            types = component['types']\n",
        "            if 'administrative_area_level_1' in types:\n",
        "                temp_state = component['long_name']\n",
        "            elif 'administrative_area_level_2' in types:\n",
        "                temp_district = component['long_name']\n",
        "            elif 'administrative_area_level_3' in types:\n",
        "                temp_subdistrict = component['long_name']\n",
        "            elif 'locality' in types:\n",
        "                temp_town_village = component['long_name']\n",
        "            elif 'sublocality' in types and not temp_town_village:\n",
        "                temp_town_village = component['long_name']\n",
        "\n",
        "        # Determine the most specific level in this result\n",
        "        if temp_town_village and found_level not in ['town_village']:\n",
        "            state = temp_state\n",
        "            district = temp_district\n",
        "            subdistrict = temp_subdistrict\n",
        "            town_village = temp_town_village\n",
        "            location = result['geometry']['location']\n",
        "            latitude = location['lat']\n",
        "            longitude = location['lng']\n",
        "            found_level = 'town_village'\n",
        "        elif temp_subdistrict and found_level not in ['town_village', 'subdistrict']:\n",
        "            state = temp_state\n",
        "            district = temp_district\n",
        "            subdistrict = temp_subdistrict\n",
        "            town_village = None\n",
        "            location = result['geometry']['location']\n",
        "            latitude = location['lat']\n",
        "            longitude = location['lng']\n",
        "            found_level = 'subdistrict'\n",
        "        elif temp_district and found_level not in ['town_village', 'subdistrict', 'district']:\n",
        "            state = temp_state\n",
        "            district = temp_district\n",
        "            subdistrict = None\n",
        "            town_village = None\n",
        "            location = result['geometry']['location']\n",
        "            latitude = location['lat']\n",
        "            longitude = location['lng']\n",
        "            found_level = 'district'\n",
        "        elif temp_state and found_level not in ['town_village', 'subdistrict', 'district', 'state']:\n",
        "            state = temp_state\n",
        "            district = None\n",
        "            subdistrict = None\n",
        "            town_village = None\n",
        "            location = result['geometry']['location']\n",
        "            latitude = location['lat']\n",
        "            longitude = location['lng']\n",
        "            found_level = 'state'\n",
        "\n",
        "        # Break the loop if the most specific level is found\n",
        "        if found_level == 'town_village':\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        'Extracted_Locations': locations,\n",
        "        'state': state,\n",
        "        'district': district,\n",
        "        'subdistrict': subdistrict,\n",
        "        'town_village': town_village,\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'location_Level': found_level,\n",
        "    }"
      ],
      "metadata": {
        "id": "r6VT1hcutZqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQ13G74YuZbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_dataframe(df, return_details, index):\n",
        "    \"\"\"Updates the DataFrame with details returned from a function.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame to update.\n",
        "        return_details: A dictionary containing the details to add.\n",
        "        index: The index of the row in the DataFrame to update.\n",
        "    \"\"\"\n",
        "    for column, value in return_details.items():\n",
        "        df.at[index, column] = value"
      ],
      "metadata": {
        "id": "T_tlyJAMSo95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_dataframe(df, model_inference_function_name, task_description):\n",
        "    \"\"\"\n",
        "    Processes a DataFrame, applying a model inference function to each row and updating the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        model_inference_function_name: The name of the function to use for model inference (e.g., get_location_details).  Must be defined in the current scope.\n",
        "        task_description: A string describing the task being performed.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing rows for {task_description}\"):\n",
        "        summary = row['Incident_Summary']\n",
        "        details = model_inference_function_name(summary) # Call the provided function\n",
        "        if details:\n",
        "            update_dataframe(df, details, index)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_str = str(datetime.timedelta(seconds=elapsed_time))\n",
        "    print(f\"Total time taken for {task_description}: {elapsed_time_str}\")\n",
        "    print(f\"{task_description} Completed  and added to the DataFrame.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5jyrjRpvX1kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "satp_data"
      ],
      "metadata": {
        "id": "5mCTtLzfubRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if satp_data is None:\n",
        "    print(\"No data\")\n",
        "    exit(1)\n",
        "else:\n",
        "    print(\"Data fetched\")\n",
        "\n",
        "    process_dataframe(satp_data, infer_perpetrator, task_description=\"Perpetrator Extraction\")\n",
        "    process_dataframe(satp_data, inference_action_type, task_description=\"Action Type Extraction\")\n",
        "    process_dataframe(satp_data, inference_target_type, task_description=\"Target Type Extraction\")\n",
        "    process_dataframe(satp_data, get_location_details, task_description=\"Location Extraction\")\n",
        "    process_dataframe(satp_data, predict_counts, task_description=\"Total Injuries, Arrests, Surrenders, Fatalities, Abducted Extraction\")\n",
        "    process_dataframe(satp_data, predict_damage, task_description=\"Damage Extraction\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiF8wB2KuFxz",
        "outputId": "b67fbe92-9d33-4bb9-bd80-4c43c952cadc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fetched from Google Sheets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows for Perpetrator Extraction: 100%|██████████| 15/15 [00:15<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken for Perpetrator Extraction: 0:00:15.624175\n",
            "Perpetrator Extraction Completed  and added to the DataFrame.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows for Action Type Extraction: 100%|██████████| 15/15 [00:17<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken for Action Type Extraction: 0:00:17.368592\n",
            "Action Type Extraction Completed  and added to the DataFrame.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows for Target Type Extraction: 100%|██████████| 15/15 [00:16<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken for Target Type Extraction: 0:00:16.960599\n",
            "Target Type Extraction Completed  and added to the DataFrame.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows for Location Extraction: 100%|██████████| 15/15 [01:10<00:00,  4.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken for Location Extraction: 0:01:10.133095\n",
            "Location Extraction Completed  and added to the DataFrame.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing rows for Total Injuries, Arrests, Surrenders, Fatalities, Abducted Extraction:   0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Processing rows for Total Injuries, Arrests, Surrenders, Fatalities, Abducted Extraction:   7%|▋         | 1/15 [00:02<00:34,  2.47s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Processing rows for Total Injuries, Arrests, Surrenders, Fatalities, Abducted Extraction: 100%|██████████| 15/15 [00:33<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken for Total Injuries, Arrests, Surrenders, Fatalities, Abducted Extraction: 0:00:33.930252\n",
            "Total Injuries, Arrests, Surrenders, Fatalities, Abducted Extraction Completed  and added to the DataFrame.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows for Damage Extraction: 100%|██████████| 15/15 [01:19<00:00,  5.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken for Damage Extraction: 0:01:19.082245\n",
            "Damage Extraction Completed  and added to the DataFrame.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}