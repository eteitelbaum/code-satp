{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and clean the data\n",
    "\n",
    "The data here are coming from the earlier zero-shot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9914 entries, 0 to 9913\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   incident_number   9913 non-null   float64\n",
      " 1   incident_summary  9914 non-null   object \n",
      " 2   original_label    9914 non-null   object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 232.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incident_number</th>\n",
       "      <th>incident_summary</th>\n",
       "      <th>original_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101010701.0</td>\n",
       "      <td>An alleged arms supplier to the Communist Part...</td>\n",
       "      <td>action undertaken by security forces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101010901.0</td>\n",
       "      <td>A Kamareddy dalam (squad) member belonging to ...</td>\n",
       "      <td>action undertaken by maoist insurgents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101030601.0</td>\n",
       "      <td>Senior CPI-Maoist 'Polit Bureau' and 'central ...</td>\n",
       "      <td>action undertaken by security forces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101051602.0</td>\n",
       "      <td>A TDP leader and former Sarpanch of Jerrela Gr...</td>\n",
       "      <td>action undertaken by maoist insurgents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101060701.0</td>\n",
       "      <td>The CPI-Maoist cadres blasted coffee pulping u...</td>\n",
       "      <td>action undertaken by maoist insurgents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   incident_number                                   incident_summary  \\\n",
       "0      101010701.0  An alleged arms supplier to the Communist Part...   \n",
       "1      101010901.0  A Kamareddy dalam (squad) member belonging to ...   \n",
       "2      101030601.0  Senior CPI-Maoist 'Polit Bureau' and 'central ...   \n",
       "3      101051602.0  A TDP leader and former Sarpanch of Jerrela Gr...   \n",
       "4      101060701.0  The CPI-Maoist cadres blasted coffee pulping u...   \n",
       "\n",
       "                           original_label  \n",
       "0    action undertaken by security forces  \n",
       "1  action undertaken by maoist insurgents  \n",
       "2    action undertaken by security forces  \n",
       "3  action undertaken by maoist insurgents  \n",
       "4  action undertaken by maoist insurgents  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import data\n",
    "satp_df = pd.read_csv('data/satp_zero_shot.csv')\n",
    "\n",
    "# remove unnecessary columns\n",
    "satp_df = satp_df.drop(['predicted_label', 'confidence'], axis=1)\n",
    "\n",
    "# view dataset info\n",
    "satp_df.info()\n",
    "satp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9914 entries, 0 to 9913\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   incident_number   9913 non-null   float64\n",
      " 1   incident_summary  9914 non-null   object \n",
      " 2   original_label    9914 non-null   object \n",
      " 3   labels            9914 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 309.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incident_number</th>\n",
       "      <th>incident_summary</th>\n",
       "      <th>original_label</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>101110701.0</td>\n",
       "      <td>Andhra Pradesh Police killed two CPI-Maoist ca...</td>\n",
       "      <td>unclear who initiated action</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>101130701.0</td>\n",
       "      <td>The Nalgonda District Police killed four CPI-M...</td>\n",
       "      <td>unclear who initiated action</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>101131201.0</td>\n",
       "      <td>A former Maoist female cadre, Veeramalla Pushp...</td>\n",
       "      <td>unclear who initiated action</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>101150901.0</td>\n",
       "      <td>A senior cadre and District committee secretar...</td>\n",
       "      <td>unclear who initiated action</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>101170801.0</td>\n",
       "      <td>Two CPI-Maoist cadres were killed in an encoun...</td>\n",
       "      <td>unclear who initiated action</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    incident_number                                   incident_summary  \\\n",
       "15      101110701.0  Andhra Pradesh Police killed two CPI-Maoist ca...   \n",
       "19      101130701.0  The Nalgonda District Police killed four CPI-M...   \n",
       "21      101131201.0  A former Maoist female cadre, Veeramalla Pushp...   \n",
       "24      101150901.0  A senior cadre and District committee secretar...   \n",
       "29      101170801.0  Two CPI-Maoist cadres were killed in an encoun...   \n",
       "\n",
       "                  original_label  labels  \n",
       "15  unclear who initiated action       2  \n",
       "19  unclear who initiated action       2  \n",
       "21  unclear who initiated action       2  \n",
       "24  unclear who initiated action       2  \n",
       "29  unclear who initiated action       2  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the labels and transform them into numeric ids\n",
    "satp_df['labels'] = label_encoder.fit_transform(satp_df['original_label'])\n",
    "\n",
    "# view dataset info\n",
    "satp_df.info()\n",
    "satp_df.head()\n",
    "satp_df['labels'].unique()\n",
    "satp_df[satp_df['labels'] == 2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50 entries, 7179 to 15\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   incident_number   50 non-null     float64\n",
      " 1   incident_summary  50 non-null     object \n",
      " 2   original_label    50 non-null     object \n",
      " 3   labels            50 non-null     int64  \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 2.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4932 entries, 6452 to 9094\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   incident_number   4932 non-null   float64\n",
      " 1   incident_summary  4932 non-null   object \n",
      " 2   original_label    4932 non-null   object \n",
      " 3   labels            4932 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 192.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4932 entries, 3007 to 8231\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   incident_number   4931 non-null   float64\n",
      " 1   incident_summary  4932 non-null   object \n",
      " 2   original_label    4932 non-null   object \n",
      " 3   labels            4932 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 192.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# import train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# select examples\n",
    "train_examples_cat1 = satp_df[satp_df['labels'] == 0].sample(n=15, random_state=42)\n",
    "train_examples_cat2 = satp_df[satp_df['labels'] == 1].sample(n=15, random_state=42)\n",
    "train_examples_cat3 = satp_df[satp_df['labels'] == 2].sample(n=20, random_state=42)\n",
    "\n",
    "# combine examples into a single data frame\n",
    "train_df = pd.concat([train_examples_cat1, train_examples_cat2, train_examples_cat3])\n",
    "\n",
    "# remove training examples from the original dataset\n",
    "remaining_df = satp_df.drop(train_df.index)\n",
    "\n",
    "# splitting the remaining examples into validation and test sets\n",
    "val_df, test_df = train_test_split(remaining_df, stratify=remaining_df['labels'], test_size=0.5, random_state=42)\n",
    "\n",
    "# view data\n",
    "train_df.info()\n",
    "test_df.info()\n",
    "val_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 3294.82 examples/s]\n",
      "Map: 100%|██████████| 4932/4932 [00:00<00:00, 10997.54 examples/s]\n",
      "Map: 100%|██████████| 4932/4932 [00:00<00:00, 11746.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['incident_summary'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True)\n",
    "test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ejt/anaconda3/envs/code-satp/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "                                               \n",
      " 33%|███▎      | 13/39 [02:07<00:08,  3.23it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0971784591674805, 'eval_accuracy': 0.41585563665855635, 'eval_f1': 0.3173054447128347, 'eval_precision': 0.39965557282889946, 'eval_recall': 0.41585563665855635, 'eval_runtime': 123.275, 'eval_samples_per_second': 40.008, 'eval_steps_per_second': 10.002, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 26/39 [04:13<00:10,  1.23it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1030911207199097, 'eval_accuracy': 0.2840632603406326, 'eval_f1': 0.25252947304732376, 'eval_precision': 0.3596577596312094, 'eval_recall': 0.2840632603406326, 'eval_runtime': 121.2052, 'eval_samples_per_second': 40.691, 'eval_steps_per_second': 10.173, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 39/39 [06:18<00:00,  9.70s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1103203296661377, 'eval_accuracy': 0.13422546634225466, 'eval_f1': 0.13391498027427287, 'eval_precision': 0.5234416685211463, 'eval_recall': 0.13422546634225466, 'eval_runtime': 120.7204, 'eval_samples_per_second': 40.855, 'eval_steps_per_second': 10.214, 'epoch': 3.0}\n",
      "{'train_runtime': 378.4163, 'train_samples_per_second': 0.396, 'train_steps_per_second': 0.103, 'train_loss': 1.095693832788712, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [01:58<00:00, 10.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1103203296661377,\n",
       " 'eval_accuracy': 0.13422546634225466,\n",
       " 'eval_f1': 0.13391498027427287,\n",
       " 'eval_precision': 0.5234416685211463,\n",
       " 'eval_recall': 0.13422546634225466,\n",
       " 'eval_runtime': 118.2419,\n",
       " 'eval_samples_per_second': 41.711,\n",
       " 'eval_steps_per_second': 10.428,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# define model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "#model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# add evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# specify training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',  # Evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "# specify training args\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,  # Add this line to include evaluation metrics\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train()\n",
    "\n",
    "# evaluate the model on the validation set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [01:58<00:00, 10.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1097095012664795,\n",
       " 'eval_accuracy': 0.14030819140308193,\n",
       " 'eval_f1': 0.14181359946531785,\n",
       " 'eval_precision': 0.6406907536398291,\n",
       " 'eval_recall': 0.14030819140308193,\n",
       " 'eval_runtime': 119.1435,\n",
       " 'eval_samples_per_second': 41.395,\n",
       " 'eval_steps_per_second': 10.349,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine-tune-perpetrator-50/tokenizer_config.json',\n",
       " './fine-tune-perpetrator-50/special_tokens_map.json',\n",
       " './fine-tune-perpetrator-50/vocab.txt',\n",
       " './fine-tune-perpetrator-50/added_tokens.json',\n",
       " './fine-tune-perpetrator-50/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./fine-tune-perpetrator-50')\n",
    "tokenizer.save_pretrained('./fine-tune-perpetrator-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use model again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./few_shot_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./few_shot_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-satp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
